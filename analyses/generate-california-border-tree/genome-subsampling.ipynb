{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import pandas as pd\n",
    "from epiweeks import Week\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Subsampling scheme for genomic database\n",
    "In order to limit the computational burden of the phylogeographic analysis, we subsampled 2500 genomes from our genomic dataset. Our subsampling scheme allocates sequences to locations based on their proximity and estimated air travel to San Diego County. Once allocated, sequences are randomly sampled proportional to the location-specific incidence data binned by epidemiological week."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We first load in the incidence data, which was downloaded and formatted by the `download-format-cases.R`. Epiweek needs to be recalculated because the `floor_date` function of the `lubridate` package generates different results from the `Week` function of the `epiweeks` library."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "      name        week  new_cases\n0  Alabama  2020-02-09          0\n1  Alabama  2020-02-16          0\n2  Alabama  2020-02-23          0\n3  Alabama  2020-03-01          0\n4  Alabama  2020-03-08         15",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>week</th>\n      <th>new_cases</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Alabama</td>\n      <td>2020-02-09</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Alabama</td>\n      <td>2020-02-16</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Alabama</td>\n      <td>2020-02-23</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Alabama</td>\n      <td>2020-03-01</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Alabama</td>\n      <td>2020-03-08</td>\n      <td>15</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = pd.read_csv( \"cases.csv\", usecols=[\"name\", \"week\", \"new_cases\"] )\n",
    "total[\"week\"] = pd.to_datetime( total[\"week\"] )\n",
    "total[\"week\"] = total[\"week\"].apply( lambda x: Week.fromdate(x).startdate() )\n",
    "total.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we load the genomic database and assign subsampling locations to each sequence. This location is state-level in Canada, Mexico, and the US, and country-level everywhere else. To prevent limited temporal sampling from biasing our analyses, we also limit our dataset to sequences collected before the last Baja California."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ct/hf5sjtrx29q3hzh3tqrd53740000gn/T/ipykernel_31347/2732893134.py:3: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  seqs = pd.read_csv( \"../../data/gisaid_metadata.csv\", parse_dates=[\"date_collected\"], usecols=usecols )\n"
     ]
    },
    {
     "data": {
      "text/plain": "             strain      accession_id date_collected pangolin_lineage  \\\n0  EPI_ISL_10022206  EPI_ISL_10022206     2022-02-02             BA.2   \n1  EPI_ISL_10022207  EPI_ISL_10022207     2022-02-02           BA.1.1   \n2  EPI_ISL_10022208  EPI_ISL_10022208     2022-02-02           BA.1.1   \n3  EPI_ISL_10022209  EPI_ISL_10022209     2022-02-01         BA.1.1.1   \n4   EPI_ISL_1002221   EPI_ISL_1002221     2021-01-08       B.1.258.17   \n\n       country division location     epiweek  ss_location  \n0      Germany      NaN      NaN  2022-01-30      Germany  \n1      Germany      NaN      NaN  2022-01-30      Germany  \n2      Germany      NaN      NaN  2022-01-30      Germany  \n3      Germany      NaN      NaN  2022-01-30      Germany  \n4  Switzerland      NaN      NaN  2021-01-03  Switzerland  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>strain</th>\n      <th>accession_id</th>\n      <th>date_collected</th>\n      <th>pangolin_lineage</th>\n      <th>country</th>\n      <th>division</th>\n      <th>location</th>\n      <th>epiweek</th>\n      <th>ss_location</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>EPI_ISL_10022206</td>\n      <td>EPI_ISL_10022206</td>\n      <td>2022-02-02</td>\n      <td>BA.2</td>\n      <td>Germany</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2022-01-30</td>\n      <td>Germany</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>EPI_ISL_10022207</td>\n      <td>EPI_ISL_10022207</td>\n      <td>2022-02-02</td>\n      <td>BA.1.1</td>\n      <td>Germany</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2022-01-30</td>\n      <td>Germany</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>EPI_ISL_10022208</td>\n      <td>EPI_ISL_10022208</td>\n      <td>2022-02-02</td>\n      <td>BA.1.1</td>\n      <td>Germany</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2022-01-30</td>\n      <td>Germany</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>EPI_ISL_10022209</td>\n      <td>EPI_ISL_10022209</td>\n      <td>2022-02-01</td>\n      <td>BA.1.1.1</td>\n      <td>Germany</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2022-01-30</td>\n      <td>Germany</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>EPI_ISL_1002221</td>\n      <td>EPI_ISL_1002221</td>\n      <td>2021-01-08</td>\n      <td>B.1.258.17</td>\n      <td>Switzerland</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2021-01-03</td>\n      <td>Switzerland</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usecols = [\"strain\", \"accession_id\", \"date_collected\", \"country\", \"division\", \"location\", \"pangolin_lineage\"]\n",
    "\n",
    "seqs = pd.read_csv( \"../../data/gisaid_metadata.csv\", parse_dates=[\"date_collected\"], usecols=usecols )\n",
    "\n",
    "# Filter probablamatic lineages\n",
    "seqs = seqs.loc[(~seqs[\"pangolin_lineage\"].isna())]\n",
    "seqs = seqs.loc[seqs[\"pangolin_lineage\"]!=\"None\"]\n",
    "seqs = seqs.loc[~seqs[\"pangolin_lineage\"].str.startswith( \"X\" )]\n",
    "\n",
    "seqs[\"epiweek\"] = seqs[\"date_collected\"].apply( lambda x: Week.fromdate(x).startdate() )\n",
    "\n",
    "# Bjorn has started replacing USA with United States, so I'll just replace it.\n",
    "seqs.loc[seqs[\"country\"]==\"United States\",\"country\"] = \"USA\"\n",
    "\n",
    "seqs[\"ss_location\"] = seqs[\"country\"]\n",
    "seqs.loc[seqs[\"ss_location\"].isin( [\"USA\", \"Mexico\", \"Canada\"] ),\"ss_location\"] = seqs[\"division\"]\n",
    "seqs.loc[seqs[\"location\"].isin( [\"San Diego\", \"San Diego County\"] ), \"ss_location\"] = \"San Diego\"\n",
    "seqs.loc[seqs[\"location\"].isin( [\"Los Angeles\", \"Los Angeles County\"] ), \"ss_location\"] = \"Los Angeles\"\n",
    "seqs[\"ss_location\"] = seqs[\"ss_location\"].astype(str).apply(lambda val: unicodedata.normalize('NFKD', val).encode('ascii', 'ignore').decode())\n",
    "\n",
    "max_bc = seqs.loc[seqs[\"ss_location\"]==\"Baja California\",\"date_collected\"].max()\n",
    "seqs = seqs.loc[seqs[\"date_collected\"]<max_bc]\n",
    "\n",
    "seqs.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We left-merge the cases onto the sequences using the epiweek and subsampling location as keys. We then convert these values to sampling weights and correct the weight for the number of sequences collected each epiweek. Because the Omicron wave resulted in a unprecedented number of cases in most locations, we downweigh periods of high cases and upweight periods of low cases by taking the square root of this weight.\n",
    "\n",
    "It should also be noted that the location and epiweek of some sequences are not present in the case data, but that these represent a relative small fraction of the entire genomic dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seqs_merged = seqs.merge( total, left_on=[\"ss_location\", \"epiweek\"], right_on=[\"name\", \"week\"], how=\"left\" )\n",
    "seqs_merged = seqs_merged.loc[~seqs_merged[\"new_cases\"].isna()]\n",
    "\n",
    "seqs_merged.loc[seqs_merged[\"new_cases\"]<0,\"new_cases\"] = 0\n",
    "seqs_merged[\"corrected\"] = seqs_merged.groupby( [\"ss_location\", \"epiweek\"] )[\"new_cases\"].transform( lambda x: x / len( x ) )\n",
    "seqs_merged[\"corrected_power\"] = seqs_merged.groupby( [\"ss_location\"] )[\"corrected\"].transform( lambda x: x / x.sum() )\n",
    "seqs_merged[\"corrected_power\"] = np.power( seqs_merged[\"corrected_power\"], 0.25 )\n",
    "\n",
    "print( seqs.shape )\n",
    "print( seqs_merged.shape )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will next allocate sequences to sample from each location. This will be a compound weight of the proximity of each location to San Diego, and the number of flights that arrived in San Diego from each location in 2019. Flight counts are used instead of mobility, which we used for other analyses, because it includes EU countries and it has a finer geographic resolution in North America.\n",
    "\n",
    "Flight data is from the OpenSky Network and is formatted using the `calculate-flights.py` script."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "flights = pd.read_csv( \"flights.csv\" )\n",
    "flights.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Distance is calculated from a shapefile of first administration regions download from https://www.naturalearthdata.com/downloads/10m-cultural-vectors/10m-admin-1-states-provinces/. Again, we calculate distances at the appropriate location levels."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "admin1 = gpd.read_file( \"/Users/natem/Documents/Data/shapefiles/ne_10m_admin_1_states_provinces/ne_10m_admin_1_states_provinces.shp\" )\n",
    "admin1 = admin1[[\"name\", \"admin\", \"geometry\"]]\n",
    "\n",
    "us_mx_ca = admin1.loc[admin1[\"admin\"].isin( [\"United States of America\", \"Mexico\", \"Canada\"])]\n",
    "\n",
    "admin = admin1.loc[~admin1[\"admin\"].isin( [\"United States of America\", \"Mexico\", \"Canada\", \"Antarctica\"]),[\"admin\", \"geometry\"]].dissolve( \"admin\" ).reset_index()\n",
    "admin.loc[admin[\"admin\"]==\"Hong Kong S.A.R.\",\"admin\"]= \"Hong Kong\"\n",
    "admin.loc[admin[\"admin\"]==\"Georgia\",\"admin\"] = \"GeorgiaC\"\n",
    "admin[\"name\"] = admin[\"admin\"]\n",
    "\n",
    "print( f\"Admin dataframe has shape: {admin.shape}\" )\n",
    "print( f\"US Mexico dataframe has shape: {us_mx_ca.shape}\" )\n",
    "\n",
    "admin = pd.concat( [admin, us_mx_ca] )\n",
    "\n",
    "print( f\"Concatenated dataframe has shape: {admin.shape}\" )\n",
    "\n",
    "# Need to convert to a flat surface so distance can be calculated. I'm using epsg2295 which is a mercator projection,\n",
    "# distance is in meters, probably, but units don't really matter.\n",
    "admin = admin.to_crs( epsg=3395 )\n",
    "sd_centroid = Point( -13001670.98, 3874670.85 )\n",
    "admin[\"distance\"] = admin.distance( sd_centroid )\n",
    "\n",
    "admin = admin.loc[~admin[\"name\"].isna()]\n",
    "admin[\"name\"] = admin[\"name\"].apply(lambda val: unicodedata.normalize('NFKD', val).encode('ascii', 'ignore').decode())\n",
    "admin = admin.drop( columns=[\"geometry\"] )\n",
    "admin[\"name\"] = admin[\"name\"].replace( { \"Mexico\":\"Estado de Mexico\", \"Distrito Federal\" : \"Mexico City\" } )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We merge flight and distance data here. To calculate weights, both data sources are normalized to have unit range. Distance data is inverted, so that closer locations have greater weight, and then taken to the 8th power, to provide relatively greater weight to closer locations. These two weights are summed, and their fraction in relation to all locations weights, are the fraction of sequences that are allocated to the location (a floor of 1 sequence is used)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merged = admin.merge( flights, left_on=\"name\", right_on=\"origin\", how=\"outer\" )\n",
    "merged = merged.loc[~merged[\"distance\"].isna()]\n",
    "merged.loc[merged[\"flights\"].isna(),\"flights\"] = 0\n",
    "\n",
    "# restrict distances to places which have sequences\n",
    "merged = merged.loc[merged[\"name\"].isin( seqs_merged[\"ss_location\"].unique() )]\n",
    "\n",
    "merged[\"normflights\"] = merged[\"flights\"] / merged[\"flights\"].max()\n",
    "merged[\"normdistance\"] = 1 - ( merged[\"distance\"] / merged[\"distance\"].max() )\n",
    "merged[\"normdistance\"] = np.power( merged[\"normdistance\"], 8 )\n",
    "merged[\"sequences\"] = merged[\"normflights\"] + merged[\"normdistance\"]\n",
    "merged[\"sequences\"] = np.round( ( merged[\"sequences\"] / merged[\"sequences\"].sum() ) * 1000 )\n",
    "\n",
    "min_value = merged.loc[merged[\"sequences\"]>0,\"sequences\"].min()\n",
    "\n",
    "seq_dict = merged.set_index( \"name\" )[\"sequences\"].to_dict()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our goal is to disect transmission within the Californias at a relative fine level. To do that we need additional location information for San Diego sequences. Here we identify San Diego sequences that have a known zipcode in the county."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "md = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/andersen-lab/HCoV-19-Genomics/master/metadata.csv\",\n",
    "    usecols=[\"ID\", \"gisaid_accession\", \"collection_date\", \"location\", \"zipcode\"],\n",
    "    dtype={\"zipcode\" : str },\n",
    "    parse_dates=[\"collection_date\"]\n",
    ")\n",
    "md[\"zipcode\"] = md[\"zipcode\"].apply( lambda x: str( x ).split( \"-\" )[0] )\n",
    "md[\"zipcode\"] = md[\"zipcode\"].str.replace(\".0\", \"\", regex=False )\n",
    "md[\"gisaid_accession\"] = md[\"gisaid_accession\"].str.strip()\n",
    "\n",
    "zips = pd.read_csv(\n",
    "    \"../../data/SanDiegoZIP_region.csv\",\n",
    "    dtype={\"ZIP\" : str}\n",
    ")\n",
    "\n",
    "md = md.merge( zips, left_on=\"zipcode\", right_on=\"ZIP\", how=\"left\" )\n",
    "acceptable_sd = md.loc[~md[\"Region\"].isna(),\"gisaid_accession\"].to_list()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we perform subsampling using the weights and allocations calculated above. 400 sequences are selected each from San Diego County, Los Angeles County, Baja California, and Quebec, Canada. Contextual sequences are sampled by iterating through each other location and sampling the allocated number of sequences. Sequences are given a sampling weight proportional to the incidence of COVID during the epidemiological week in which they were collected.\n",
    "\n",
    "To support the root of the phylogeny we also include the 50 earliest sequences and the 50 earliest European sequences in the dataset. Lastly, to assess the temporal reconstruction of the phylogeny, we also include genomes from three outbreaks with well-described introductions.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seqs_merged.loc[seqs_merged[\"ss_location\"]==\"San Diego\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "focus = [\"Baja California\", \"San Diego\", \"Los Angeles\"]\n",
    "\n",
    "specified = seqs_merged.loc[seqs_merged[\"ss_location\"].isin( focus )]\n",
    "contextual = seqs_merged.loc[~seqs_merged[\"ss_location\"].isin( focus )]\n",
    "\n",
    "# Contextual\n",
    "selected_df = list()\n",
    "for name, df in contextual.groupby( \"ss_location\" ):\n",
    "    try:\n",
    "        requested = int( seq_dict[name] )\n",
    "    except KeyError:\n",
    "        #print( f\"no entry for {name}. Skipping...\" )\n",
    "        requested = 0\n",
    "\n",
    "    requested = min( requested, len( df ) )\n",
    "    try:\n",
    "        sampled = df.sample( n=requested, replace=False, weights=\"corrected_power\" )\n",
    "        sampled[\"role\"] = \"contextual\"\n",
    "        selected_df.append( sampled )\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "# Focal\n",
    "for name, df in specified.groupby( \"ss_location\" ):\n",
    "    if name == \"San Diego\":\n",
    "        sampled = df.loc[df[\"accession_id\"].isin( acceptable_sd )].sample( n=500, replace=False, weights=\"corrected_power\" )\n",
    "    else:\n",
    "        sampled = df.sample( n=500, replace=False, weights=\"corrected_power\" )\n",
    "    sampled[\"role\"] = name\n",
    "    selected_df.append( sampled )\n",
    "\n",
    "# Earliest European\n",
    "with open( \"worobey_early.txt\", \"r\" ) as f:\n",
    "    early = [line.strip() for line in f]\n",
    "sampled = seqs_merged.loc[seqs_merged[\"accession_id\"].isin( early )]\n",
    "sampled[\"role\"] = \"earliest_european\"\n",
    "selected_df.append( sampled )\n",
    "\n",
    "# Known outbreaks\n",
    "with open( \"known_clades.txt\", \"r\" ) as f:\n",
    "    clades = [line.strip() for line in f if not line.startswith( \"#\" )]\n",
    "sampled = seqs_merged.loc[seqs_merged[\"accession_id\"].isin( clades )]\n",
    "sampled[\"role\"] = \"known_clades\"\n",
    "selected_df.append( sampled )\n",
    "\n",
    "# Earliest representatives of VOCs\n",
    "with open( \"early_lineages.txt\", \"r\" ) as f:\n",
    "    early_lin = [line.strip() for line in f if not line.startswith( \"#\" )]\n",
    "sampled = seqs_merged.loc[seqs_merged[\"accession_id\"].isin( early_lin )]\n",
    "sampled[\"role\"] = \"lineage_representatives\"\n",
    "selected_df.append( sampled )\n",
    "\n",
    "selected_df = pd.concat( selected_df )\n",
    "print( f\"{selected_df['week'].min()} - {selected_df['week'].max()}\")\n",
    "\n",
    "print( len( selected_df ) )\n",
    "selected_df.to_csv( \"selected_2022-11-17.csv\" )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ct/hf5sjtrx29q3hzh3tqrd53740000gn/T/ipykernel_31347/816169508.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sampled[\"role\"] = \"earliest_european\"\n",
      "/var/folders/ct/hf5sjtrx29q3hzh3tqrd53740000gn/T/ipykernel_31347/816169508.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sampled[\"role\"] = \"known_clades\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-09 - 2022-10-16\n",
      "2596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ct/hf5sjtrx29q3hzh3tqrd53740000gn/T/ipykernel_31347/816169508.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sampled[\"role\"] = \"lineage_representatives\"\n"
     ]
    }
   ],
   "source": [
    "focus = [\"Baja California\", \"San Diego\", \"Los Angeles\"]\n",
    "\n",
    "specified = seqs_merged.loc[seqs_merged[\"ss_location\"].isin( focus )]\n",
    "contextual = seqs_merged.loc[~seqs_merged[\"ss_location\"].isin( focus )]\n",
    "\n",
    "# Contextual\n",
    "selected_df = list()\n",
    "for name, df in contextual.groupby( \"ss_location\" ):\n",
    "    try:\n",
    "        requested = int( seq_dict[name] )\n",
    "    except KeyError:\n",
    "        #print( f\"no entry for {name}. Skipping...\" )\n",
    "        requested = 0\n",
    "\n",
    "    requested = min( requested, len( df ) )\n",
    "    try:\n",
    "        sampled = df.sample( n=requested, replace=False, weights=\"corrected_power\" )\n",
    "        sampled[\"role\"] = \"contextual\"\n",
    "        selected_df.append( sampled )\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "# Focal\n",
    "for name, df in specified.groupby( \"ss_location\" ):\n",
    "    if name == \"San Diego\":\n",
    "        sampled = df.loc[df[\"accession_id\"].isin( acceptable_sd )].sample( n=500, replace=False, weights=\"corrected_power\" )\n",
    "    else:\n",
    "        sampled = df.sample( n=500, replace=False, weights=\"corrected_power\" )\n",
    "    sampled[\"role\"] = name\n",
    "    selected_df.append( sampled )\n",
    "\n",
    "# Earliest European\n",
    "with open( \"worobey_early.txt\", \"r\" ) as f:\n",
    "    early = [line.strip() for line in f]\n",
    "sampled = seqs_merged.loc[seqs_merged[\"accession_id\"].isin( early )]\n",
    "sampled[\"role\"] = \"earliest_european\"\n",
    "selected_df.append( sampled )\n",
    "\n",
    "# Known outbreaks\n",
    "with open( \"known_clades.txt\", \"r\" ) as f:\n",
    "    clades = [line.strip() for line in f if not line.startswith( \"#\" )]\n",
    "sampled = seqs_merged.loc[seqs_merged[\"accession_id\"].isin( clades )]\n",
    "sampled[\"role\"] = \"known_clades\"\n",
    "selected_df.append( sampled )\n",
    "\n",
    "# Earliest representatives of VOCs\n",
    "with open( \"early_lineages.txt\", \"r\" ) as f:\n",
    "    early_lin = [line.strip() for line in f if not line.startswith( \"#\" )]\n",
    "sampled = seqs_merged.loc[seqs_merged[\"accession_id\"].isin( early_lin )]\n",
    "sampled[\"role\"] = \"lineage_representatives\"\n",
    "selected_df.append( sampled )\n",
    "\n",
    "selected_df = pd.concat( selected_df )\n",
    "print( f\"{selected_df['week'].min()} - {selected_df['week'].max()}\")\n",
    "\n",
    "print( len( selected_df ) )\n",
    "selected_df.to_csv( \"selected_2022-11-17.csv\" )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "San Diego          510\nLos Angeles        500\nBaja California    500\nCalifornia          36\nLouisiana           28\n                  ... \nMorocco              1\nUruguay              1\nParaguay             1\nSpain                1\nBotswana             1\nName: ss_location, Length: 137, dtype: int64"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_df[\"ss_location\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "North Central    116\nEast             109\nSouth            102\nCentral           93\nNorth Inland      53\nNorth Coastal     34\nName: Region, dtype: int64"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md.loc[md[\"gisaid_accession\"].isin( selected_df.loc[selected_df[\"ss_location\"]==\"San Diego\",\"accession_id\"].to_list()),\"Region\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}