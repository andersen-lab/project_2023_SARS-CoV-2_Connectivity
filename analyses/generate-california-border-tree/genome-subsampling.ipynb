{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import pandas as pd\n",
    "from epiweeks import Week\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Subsampling scheme for genomic database\n",
    "In order to limit the computational burden of the phylogeographic analysis, we subsampled 2500 genomes from our genomic dataset. Our subsampling scheme allocates sequences to locations based on their proximity and estimated air travel to San Diego County. Once allocated, sequences are randomly sampled proportional to the location-specific incidence data binned by epidemiological week."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We first load in the incidence data, which was downloaded and formatted by the `download-format-cases.R`. Epiweek needs to be recalculated because the `floor_date` function of the `lubridate` package generates different results from the `Week` function of the `epiweeks` library."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "      name        week  new_cases\n0  Alabama  2020-02-09          0\n1  Alabama  2020-02-16          0\n2  Alabama  2020-02-23          0\n3  Alabama  2020-03-01          0\n4  Alabama  2020-03-08         15",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>week</th>\n      <th>new_cases</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Alabama</td>\n      <td>2020-02-09</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Alabama</td>\n      <td>2020-02-16</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Alabama</td>\n      <td>2020-02-23</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Alabama</td>\n      <td>2020-03-01</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Alabama</td>\n      <td>2020-03-08</td>\n      <td>15</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = pd.read_csv( \"cases.csv\", usecols=[\"name\", \"week\", \"new_cases\"] )\n",
    "total[\"week\"] = pd.to_datetime( total[\"week\"] )\n",
    "total[\"week\"] = total[\"week\"].apply( lambda x: Week.fromdate(x).startdate() )\n",
    "total.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we load the genomic database and assign subsampling locations to each sequence. This location is state-level in Canada, Mexico, and the US, and country-level everywhere else. To prevent limited temporal sampling from biasing our analyses, we also limit our dataset to sequences collected before the last Baja California."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "                                       strain gisaid_accession date_collected  \\\n0  AFG|Afghanistan|EPI_ISL_1034760|2020-06-06  EPI_ISL_1034760     2020-06-06   \n1  AFG|Afghanistan|EPI_ISL_1000998|2020-06-13  EPI_ISL_1000998     2020-06-13   \n2  AFG|Afghanistan|EPI_ISL_1000999|2020-06-07  EPI_ISL_1000999     2020-06-07   \n3  AFG|Afghanistan|EPI_ISL_1001000|2020-06-02  EPI_ISL_1001000     2020-06-02   \n4  AFG|Afghanistan|EPI_ISL_1001001|2020-05-30  EPI_ISL_1001001     2020-05-30   \n\n  region      country     division location   host pangolin_lineage  \\\n0   Asia  Afghanistan  Afghanistan     None  Human            B.1.1   \n1   Asia  Afghanistan  Afghanistan     None  Human           B.1.36   \n2   Asia  Afghanistan  Afghanistan     None  Human            B.1.9   \n3   Asia  Afghanistan  Afghanistan     None  Human              B.1   \n4   Asia  Afghanistan  Afghanistan     None  Human              B.1   \n\n      epiweek  ss_location  \n0  2020-05-31  Afghanistan  \n1  2020-06-07  Afghanistan  \n2  2020-06-07  Afghanistan  \n3  2020-05-31  Afghanistan  \n4  2020-05-24  Afghanistan  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>strain</th>\n      <th>gisaid_accession</th>\n      <th>date_collected</th>\n      <th>region</th>\n      <th>country</th>\n      <th>division</th>\n      <th>location</th>\n      <th>host</th>\n      <th>pangolin_lineage</th>\n      <th>epiweek</th>\n      <th>ss_location</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AFG|Afghanistan|EPI_ISL_1034760|2020-06-06</td>\n      <td>EPI_ISL_1034760</td>\n      <td>2020-06-06</td>\n      <td>Asia</td>\n      <td>Afghanistan</td>\n      <td>Afghanistan</td>\n      <td>None</td>\n      <td>Human</td>\n      <td>B.1.1</td>\n      <td>2020-05-31</td>\n      <td>Afghanistan</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>AFG|Afghanistan|EPI_ISL_1000998|2020-06-13</td>\n      <td>EPI_ISL_1000998</td>\n      <td>2020-06-13</td>\n      <td>Asia</td>\n      <td>Afghanistan</td>\n      <td>Afghanistan</td>\n      <td>None</td>\n      <td>Human</td>\n      <td>B.1.36</td>\n      <td>2020-06-07</td>\n      <td>Afghanistan</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>AFG|Afghanistan|EPI_ISL_1000999|2020-06-07</td>\n      <td>EPI_ISL_1000999</td>\n      <td>2020-06-07</td>\n      <td>Asia</td>\n      <td>Afghanistan</td>\n      <td>Afghanistan</td>\n      <td>None</td>\n      <td>Human</td>\n      <td>B.1.9</td>\n      <td>2020-06-07</td>\n      <td>Afghanistan</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>AFG|Afghanistan|EPI_ISL_1001000|2020-06-02</td>\n      <td>EPI_ISL_1001000</td>\n      <td>2020-06-02</td>\n      <td>Asia</td>\n      <td>Afghanistan</td>\n      <td>Afghanistan</td>\n      <td>None</td>\n      <td>Human</td>\n      <td>B.1</td>\n      <td>2020-05-31</td>\n      <td>Afghanistan</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>AFG|Afghanistan|EPI_ISL_1001001|2020-05-30</td>\n      <td>EPI_ISL_1001001</td>\n      <td>2020-05-30</td>\n      <td>Asia</td>\n      <td>Afghanistan</td>\n      <td>Afghanistan</td>\n      <td>None</td>\n      <td>Human</td>\n      <td>B.1</td>\n      <td>2020-05-24</td>\n      <td>Afghanistan</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usecols = [\"strain\", \"gisaid_accession\", \"date_collected\", \"region\", \"country\", \"division\", \"location\", \"host\", \"pangolin_lineage\"]\n",
    "\n",
    "seqs = pd.read_csv( \"/Users/natem/Dropbox (Scripps Research)/Personal/Code/Projects/project_2021_california-hcov-genomics/data/metadata_filtered.tsv\", sep=\"\\t\", parse_dates=[\"date_collected\"], usecols=usecols )\n",
    "seqs = seqs.loc[(~seqs[\"pangolin_lineage\"].isna())&(seqs[\"host\"]==\"Human\")]\n",
    "seqs[\"epiweek\"] = seqs[\"date_collected\"].apply( lambda x: Week.fromdate(x).startdate() )\n",
    "\n",
    "seqs[\"ss_location\"] = seqs[\"country\"]\n",
    "seqs.loc[seqs[\"ss_location\"].isin( [\"USA\", \"Mexico\", \"Canada\"] ),\"ss_location\"] = seqs[\"division\"]\n",
    "seqs.loc[seqs[\"location\"].isin( [\"San Diego\", \"San Diego County\"] ), \"ss_location\"] = \"San Diego\"\n",
    "seqs.loc[seqs[\"location\"].isin( [\"Los Angeles\", \"Los Angeles County\"] ), \"ss_location\"] = \"Los Angeles\"\n",
    "seqs[\"ss_location\"] = seqs[\"ss_location\"].apply(lambda val: unicodedata.normalize('NFKD', val).encode('ascii', 'ignore').decode())\n",
    "\n",
    "max_bc = seqs.loc[seqs[\"ss_location\"]==\"Baja California\",\"date_collected\"].max()\n",
    "seqs = seqs.loc[seqs[\"date_collected\"]<max_bc]\n",
    "\n",
    "seqs.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We left-merge the cases onto the sequences using the epiweek and subsampling location as keys. We then convert these values to sampling weights and correct the weight for the number of sequences collected each epiweek.\n",
    "\n",
    "It should also be noted that the location and epiweek of some sequences are not present in the case data, but that these represent a relative small fraction of the entire genomic dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2487539, 11)\n",
      "(2318356, 16)\n"
     ]
    }
   ],
   "source": [
    "seqs_merged = seqs.merge( total, left_on=[\"ss_location\", \"epiweek\"], right_on=[\"name\", \"week\"], how=\"left\" )\n",
    "seqs_merged = seqs_merged.loc[~seqs_merged[\"new_cases\"].isna()]\n",
    "\n",
    "seqs_merged.loc[seqs_merged[\"new_cases\"]<0,\"new_cases\"] = 0\n",
    "seqs_merged[\"corrected\"] = seqs_merged.groupby( [\"location\", \"epiweek\"] )[\"new_cases\"].transform( lambda x: x / len( x ) )\n",
    "seqs_merged[\"probability\"] = seqs_merged[\"new_cases\"] / seqs_merged[\"new_cases\"].sum()\n",
    "\n",
    "print( seqs.shape )\n",
    "print( seqs_merged.shape )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will next allocate sequences to sample from each location. This will be a compound weight of the proximity of each location to San Diego, and the number of flights destined for San Diego from each location in 2019.\n",
    "\n",
    "Flight data is from the OpenSky Network and is formatted using the `calculate-flights.py` script."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "     origin  flights\n0   Alabama       17\n1    Alaska        9\n2   Alberta      182\n3   Arizona     6089\n4  Arkansas        9",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>origin</th>\n      <th>flights</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Alabama</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Alaska</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Alberta</td>\n      <td>182</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Arizona</td>\n      <td>6089</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Arkansas</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights = pd.read_csv( \"flights.csv\" )\n",
    "flights.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Distance is calculated from a shapefile of first administration regions download from https://www.naturalearthdata.com/downloads/10m-cultural-vectors/10m-admin-1-states-provinces/. Again, we calculate distances at the appropriate location levels."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Admin dataframe has shape: (249, 3)\n",
      "US Mexico dataframe has shape: (97, 3)\n",
      "Concatenated dataframe has shape: (346, 3)\n"
     ]
    }
   ],
   "source": [
    "admin1 = gpd.read_file( \"/Users/natem/Documents/Data/shapefiles/ne_10m_admin_1_states_provinces/ne_10m_admin_1_states_provinces.shp\" )\n",
    "admin1 = admin1[[\"name\", \"admin\", \"geometry\"]]\n",
    "\n",
    "us_mx_ca = admin1.loc[admin1[\"admin\"].isin( [\"United States of America\", \"Mexico\", \"Canada\"])]\n",
    "\n",
    "admin = admin1.loc[~admin1[\"admin\"].isin( [\"United States of America\", \"Mexico\", \"Canada\", \"Antarctica\"]),[\"admin\", \"geometry\"]].dissolve( \"admin\" ).reset_index()\n",
    "admin.loc[admin[\"admin\"]==\"Hong Kong S.A.R.\",\"admin\"]= \"Hong Kong\"\n",
    "admin.loc[admin[\"admin\"]==\"Georgia\",\"admin\"] = \"GeorgiaC\"\n",
    "admin[\"name\"] = admin[\"admin\"]\n",
    "\n",
    "print( f\"Admin dataframe has shape: {admin.shape}\" )\n",
    "print( f\"US Mexico dataframe has shape: {us_mx_ca.shape}\" )\n",
    "\n",
    "admin = pd.concat( [admin, us_mx_ca] )\n",
    "\n",
    "print( f\"Concatenated dataframe has shape: {admin.shape}\" )\n",
    "\n",
    "# Need to convert to a flat surface so distance can be calculated. I'm using epsg2295 which is a mercator projection,\n",
    "# distance is in meters, probably, but units don't really matter.\n",
    "admin = admin.to_crs( epsg=3395 )\n",
    "sd_centroid = Point( -13001670.98, 3874670.85 )\n",
    "admin[\"distance\"] = admin.distance( sd_centroid )\n",
    "\n",
    "admin = admin.loc[~admin[\"name\"].isna()]\n",
    "admin[\"name\"] = admin[\"name\"].apply(lambda val: unicodedata.normalize('NFKD', val).encode('ascii', 'ignore').decode())\n",
    "admin = admin.drop( columns=[\"geometry\"] )\n",
    "admin[\"name\"] = admin[\"name\"].replace( { \"Mexico\":\"Estado de Mexico\", \"Distrito Federal\" : \"Mexico City\" } )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We merge flight and distance data here. To calculate weights, both data sources are normalized to have unit range. Distance data is inverted, so that closer locations have greater weight, and then taken to the 8th power, to provide relatively greater weight to closer locations. These two weights are summed, and their fraction in relation to all locations weights, are the fraction of sequences that are allocated to the location (a floor of 1 sequence is used)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "merged = admin.merge( flights, left_on=\"name\", right_on=\"origin\", how=\"outer\" )\n",
    "merged = merged.loc[~merged[\"distance\"].isna()]\n",
    "merged.loc[merged[\"flights\"].isna(),\"flights\"] = 0\n",
    "\n",
    "# restrict distances to places which have sequences\n",
    "merged = merged.loc[merged[\"name\"].isin( seqs_merged[\"ss_location\"].unique() )]\n",
    "\n",
    "merged[\"normflights\"] = merged[\"flights\"] / merged[\"flights\"].max()\n",
    "merged[\"normdistance\"] = 1 - ( merged[\"distance\"] / merged[\"distance\"].max() )\n",
    "merged[\"normdistance\"] = np.power( merged[\"normdistance\"], 8 )\n",
    "merged[\"sequences\"] = merged[\"normflights\"] + merged[\"normdistance\"]\n",
    "merged[\"sequences\"] = np.round( ( merged[\"sequences\"] / merged[\"sequences\"].sum() ) * 1000 )\n",
    "\n",
    "min_value = merged.loc[merged[\"sequences\"]>0,\"sequences\"].min()\n",
    "\n",
    "seq_dict = merged.set_index( \"name\" )[\"sequences\"].to_dict()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we perform subsampling using the weights and allocations calculated above. 400 sequences are selected each from San Diego County, Los Angeles County, Baja California, and Quebec, Canada. Contextual sequences are sampled by iterating through each other location and sampling the allocated number of sequences. Sequences are given a sampling weight proportional to the incidence of COVID during the epidemiological week in which they were collected.\n",
    "\n",
    "To support the root of the phylogeny we also include the 50 earliest sequences and the 50 earliest European sequences in the dataset. Lastly, to assess the temporal reconstruction of the phylogeny, we also include genomes from three outbreaks with well-described introductions.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "San Diego          15558\nQuebec             14185\nLos Angeles         5744\nBaja California     1251\nName: ss_location, dtype: int64"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs_merged.loc[seqs_merged[\"ss_location\"].isin([\"Baja California\", \"San Diego\", \"Los Angeles\", \"Quebec\"]),\"ss_location\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ct/hf5sjtrx29q3hzh3tqrd53740000gn/T/ipykernel_99209/3455392776.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sampled[\"role\"] = \"earliest_european\"\n",
      "/var/folders/ct/hf5sjtrx29q3hzh3tqrd53740000gn/T/ipykernel_99209/3455392776.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sampled[\"role\"] = \"known_clades\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-09 - 2021-07-11\n",
      "2647\n"
     ]
    }
   ],
   "source": [
    "focus = [\"Baja California\", \"San Diego\", \"Los Angeles\", \"Quebec\"]\n",
    "\n",
    "specified = seqs_merged.loc[seqs_merged[\"ss_location\"].isin( focus )]\n",
    "contextual = seqs_merged.loc[~seqs_merged[\"ss_location\"].isin( focus )]\n",
    "\n",
    "# Contextual\n",
    "selected_df = list()\n",
    "for name, df in contextual.groupby( \"ss_location\" ):\n",
    "    try:\n",
    "        requested = int( seq_dict[name] )\n",
    "    except KeyError:\n",
    "        #print( f\"no entry for {name}. Skipping...\" )\n",
    "        requested = 0\n",
    "\n",
    "    requested = min( requested, len( df ) )\n",
    "    try:\n",
    "        sampled = df.sample( n=requested, replace=False, weights=\"corrected\" )\n",
    "        sampled[\"role\"] = \"contextual\"\n",
    "        selected_df.append( sampled )\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "# Focal\n",
    "for name, df in specified.groupby( \"ss_location\" ):\n",
    "    sampled = df.sample( n=400, replace=False, weights=\"corrected\" )\n",
    "    sampled[\"role\"] = name\n",
    "    selected_df.append( sampled )\n",
    "\n",
    "# Earliest\n",
    "# sampled = seqs_merged.sort_values(\"date_collected\").head(50)\n",
    "# sampled[\"role\"] = \"earliest\"\n",
    "# selected_df.append( sampled )\n",
    "\n",
    "# Earliest European\n",
    "with open( \"worobey_early.txt\", \"r\" ) as f:\n",
    "    early = [line.strip() for line in f]\n",
    "sampled = seqs_merged.loc[seqs_merged[\"gisaid_accession\"].isin( early )]\n",
    "sampled[\"role\"] = \"earliest_european\"\n",
    "selected_df.append( sampled )\n",
    "\n",
    "# Known outbreaks\n",
    "with open( \"known_clades.txt\", \"r\" ) as f:\n",
    "    clades = [line.strip() for line in f if not line.startswith( \"#\" )]\n",
    "sampled = seqs_merged.loc[seqs_merged[\"gisaid_accession\"].isin( clades )]\n",
    "sampled[\"role\"] = \"known_clades\"\n",
    "selected_df.append( sampled )\n",
    "\n",
    "selected_df = pd.concat( selected_df )\n",
    "print( f\"{selected_df['week'].min()} - {selected_df['week'].max()}\")\n",
    "\n",
    "print( len( selected_df ) )\n",
    "selected_df.to_csv( \"selected_2022-06-23.csv\" )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}